{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvHDeZP5ICUh"
      },
      "source": [
        "This Python script is designed for web scraping real estate listings from a popular Polish website. It fetches multiple pages of real estate offers using requests and BeautifulSoup, extracts key listing details from embedded JSON data, and structures them into a DataFrame. The extracted data is then cleaned, anonymized, and saved as CSV files in Google Drive. Additionally, the script includes functionality to retrieve detailed information for each listing, ensuring comprehensive property data collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZr7nkm7c85Z"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZIWHslXc661",
        "outputId": "40e62e45-099e-499e-c8e0-14c9b5d2300f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from time import sleep\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wsn0xRhdDHB"
      },
      "source": [
        "# Scrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eveWbHZTeDtP"
      },
      "outputs": [],
      "source": [
        "def get_offers(page_number):\n",
        "  \"\"\"\n",
        "  Fetches offers from a specific page on real estate website for apartments for sale.\n",
        "  Args:\n",
        "      page_number (int): The page number to fetch the offers from.\n",
        "  Returns:\n",
        "      list: A list of offer objects containing data about real estate listings.\n",
        "  \"\"\"\n",
        "  website = ... # Url to one of the most popular websites in Poland\n",
        "  # URL for the specific page number in search results\n",
        "  url = f\"{website}{page_number}\"\n",
        "\n",
        "  # HTTP headers to simulate a request from a standard web browser\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "  # Send an HTTP GET request to the specified URL\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  # Introduce a 1-second delay to avoid overwhelming the server\n",
        "  sleep(1)\n",
        "\n",
        "  # Get the HTML content of the response\n",
        "  content = response.text  # The response contains JSON embedded within the HTML\n",
        "\n",
        "  # Parse the HTML content using BeautifulSoup\n",
        "  soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "  # Extract the script tag containing the JSON data (with the ID \"__NEXT_DATA__\")\n",
        "  text = soup.find('script', {'id': \"__NEXT_DATA__\"}).get_text()\n",
        "\n",
        "  # Parse the JSON content into a Python dictionary\n",
        "  data_json = json.loads(text)\n",
        "\n",
        "  # Navigate to the specific key where offer data is stored\n",
        "  offers = data_json['props']['pageProps']['data']['searchAds']['items']\n",
        "\n",
        "  # Return the list of offers\n",
        "  return offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owAWnj08k2KY"
      },
      "outputs": [],
      "source": [
        "def parse_offer(offer):\n",
        "  \"\"\"\n",
        "  Parses a single offer from the real estate website data structure into a clean, structured dictionary.\n",
        "  Args:\n",
        "      offer (dict): The raw offer data from the Otodom API response.\n",
        "  Returns:\n",
        "      dict: A parsed and structured dictionary containing key offer details.\n",
        "  \"\"\"\n",
        "  website_prefix = ...\n",
        "  website_prefix_dev = ...\n",
        "\n",
        "  # Construct the main offer URL using the \"slug\" field\n",
        "  url = website_prefix + offer[\"slug\"]\n",
        "\n",
        "  # Try to construct the developer URL; fallback to \"N/A\" if not available\n",
        "  try:\n",
        "    developer_url = website_prefix_dev + offer[\"agency\"][\"slug\"]\n",
        "  except:\n",
        "    developer_url = \"N/A\"\n",
        "\n",
        "  # Extract and clean the creation dates\n",
        "  dateCreated = offer[\"dateCreated\"].split(\" \")[0]\n",
        "  dateCreatedFirst = offer[\"dateCreatedFirst\"].split(\" \")[0]\n",
        "\n",
        "  # Define keys for direct extraction and nested key paths for deeper extraction\n",
        "  keys = [\"id\", \"title\", \"estate\", \"developmentId\", \"transaction\", \"areaInSquareMeters\", \"roomsNumber\"]\n",
        "  nested_keys = {\n",
        "    \"location\": [\"location\", \"reverseGeocoding\", \"locations\", 3, \"fullName\"],\n",
        "    \"agencyId\": [\"agency\", \"id\"],\n",
        "    \"agencyName\": [\"agency\", \"name\"],\n",
        "    \"price\": [\"totalPrice\", \"value\"]\n",
        "  }\n",
        "\n",
        "  # Extract simple keys from the offer data\n",
        "  parsed_offer = {key: offer.get(key, \"N/A\") for key in keys}\n",
        "\n",
        "  # Add basic information and URLs to the parsed offer\n",
        "  parsed_offer[\"url\"] = url\n",
        "  parsed_offer[\"developerUrl\"] = developer_url\n",
        "  parsed_offer[\"dateModified\"] = dateCreated\n",
        "  parsed_offer[\"dateOfCreation\"] = dateCreatedFirst\n",
        "\n",
        "  # Extract nested keys from the offer data using safe acces function\n",
        "  parsed_data_custom = {\n",
        "    key: safe_acces(offer, path)\n",
        "    for key, path in nested_keys.items()\n",
        "  }\n",
        "  parsed_offer.update(parsed_data_custom)\n",
        "\n",
        "  return parsed_offer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbC3DkNhHWqD"
      },
      "outputs": [],
      "source": [
        "def safe_acces(data, path, default=\"N/A\"):\n",
        "  \"\"\"\n",
        "  Safely traverses a nested dictionary or list to extract a value.\n",
        "  Args:\n",
        "      data (dict or list): The initial data structure to traverse.\n",
        "      path (list): A list of keys (for dictionaries) or indices (for lists) representing the path to the desired value.\n",
        "      default: The value to return if any part of the path is invalid or the data at the end of the path is missing.\n",
        "  Returns:\n",
        "      The value at the end of the path if it exists, or the default value if any part of the path is invalid.\n",
        "  \"\"\"\n",
        "  for key in path:\n",
        "    try:\n",
        "      # If the current level is a dictionary, use .get() for safe access\n",
        "      if isinstance(data, dict):\n",
        "        data = data.get(key)\n",
        "      # If the current level is a list, access by index if valid\n",
        "      elif isinstance(data, list):\n",
        "        data = data[key] if 0 <= key < len(data) else default\n",
        "      else:\n",
        "        # If the data is neither a dict nor a list, return default\n",
        "        return default\n",
        "    except (TypeError, IndexError):\n",
        "      # Catch errors caused by invalid key/index access or wrong types\n",
        "      return default\n",
        "\n",
        "  # Return the value if found, otherwise fall back to the default\n",
        "  return data or default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b4el-DQc-Cc"
      },
      "outputs": [],
      "source": [
        "def getter():\n",
        "  \"\"\"\n",
        "  Retrieves and parses all offers from the  page of real estate search results.\n",
        "  Returns:\n",
        "      list: A list of parsed offers, each represented as a structured dictionary.\n",
        "  \"\"\"\n",
        "  # Initialize an empty list to store all parsed offers\n",
        "  all_offers = []\n",
        "  pages_number = ...\n",
        "\n",
        "  # Loop through pages\n",
        "  for page_number in range(0, pages_number):\n",
        "    # Fetch offers from the current page\n",
        "    offers = get_offers(page_number)\n",
        "    print(f\"Downloading {page_number} of {pages_number-1}\")\n",
        "\n",
        "    # Parse each offer and add it to the list of all offers\n",
        "    for i, offer in enumerate(offers):\n",
        "      all_offers.append(parse_offer(offer))\n",
        "\n",
        "  # Return the complete list of parsed offers\n",
        "  return all_offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv2j2TpvgiGt",
        "outputId": "1f2972b5-b487-4eba-ac62-b59f7a85f833"
      },
      "outputs": [],
      "source": [
        "# Convert the list of parsed offers into a DataFrame and set \"id\" as the index\n",
        "df = pd.DataFrame(getter()).set_index(\"id\")\n",
        "\n",
        "# Remove duplicate entries based on the \"id\" index, keeping the first occurrence\n",
        "df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_df.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf7Ug0ivcFE2"
      },
      "outputs": [],
      "source": [
        "def get_offer_details(url):\n",
        "  \"\"\"\n",
        "  Retrieves detailed information about a specific offer from its URL.\n",
        "  Args:\n",
        "      url (str): The URL of the offer to fetch details for.\n",
        "  Returns:\n",
        "      dict: A dictionary containing detailed data about the offer.\n",
        "  \"\"\"\n",
        "  # HTTP headers to simulate a request from a standard web browser\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "  # Send an HTTP GET request to fetch the offer page\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  # Introduce a 1-second delay to avoid overwhelming the server\n",
        "  sleep(1)\n",
        "\n",
        "  # Extract the HTML content of the response\n",
        "  content = response.text\n",
        "\n",
        "  # Parse the HTML content using BeautifulSoup\n",
        "  soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "  # Extract the script tag containing the JSON data (with the ID \"__NEXT_DATA__\")\n",
        "  text = soup.find('script', {'id': \"__NEXT_DATA__\"}).get_text()\n",
        "\n",
        "  # Parse the JSON content into a Python dictionary\n",
        "  data_json = json.loads(text)\n",
        "\n",
        "  # Navigate to the key where the offer details are stored\n",
        "  offers = data_json[\"props\"][\"pageProps\"][\"ad\"]\n",
        "\n",
        "  # Return the detailed offer data\n",
        "  return offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNCBzDASeW9l"
      },
      "outputs": [],
      "source": [
        "def parse_offer_details(offer):\n",
        "  \"\"\"\n",
        "  Parses detailed information about a specific offer into a structured dictionary.\n",
        "  Args:\n",
        "      offer (dict): The raw detailed offer data.\n",
        "  Returns:\n",
        "      dict: A parsed dictionary containing key details about the offer.\n",
        "  \"\"\"\n",
        "  # Direct keys to extract from the offer\n",
        "  keys = [\"id\", \"market\", \"advertiserType\", \"advertType\"]\n",
        "\n",
        "  # Nested keys to extract with their respective paths in the offer dictionary\n",
        "  nested_keys = {\n",
        "    \"buildYear\": [\"target\", \"Build_year\"],\n",
        "    \"buildingFloorsNum\": [\"target\", \"Building_floors_num\"],\n",
        "    \"cityId\": [\"target\", \"City_id\"],\n",
        "    \"buildingOwnership\": [\"target\", \"Building_ownership\"],\n",
        "    \"buildingType\": [\"target\", \"Building_type\"],\n",
        "    \"constructionStatus\": [\"target\", \"Construction_status\"],\n",
        "    \"extrasTypes\": [\"target\", \"Extras_types\"],\n",
        "    \"latitude\": [\"location\", \"coordinates\", \"latitude\"],\n",
        "    \"longitude\": [\"location\", \"coordinates\", \"longitude\"]\n",
        "  }\n",
        "\n",
        "  # Extract direct keys\n",
        "  parsed_offer = {key: offer.get(key, \"N/A\") for key in keys}\n",
        "\n",
        "  # Extract nested keys using the safe_acces function\n",
        "  parsed_data_custom = {\n",
        "    key: safe_acces(offer, path)\n",
        "    for key, path in nested_keys.items()\n",
        "  }\n",
        "  parsed_offer.update(parsed_data_custom)\n",
        "\n",
        "  # Additional fields requiring custom extraction and transformation\n",
        "  extraction_config = {\n",
        "    \"outdoor\": ([\"topInformation\", 5, \"values\", 0], lambda x: x.split(\"::\")[1]),\n",
        "    \"heating\": ([\"topInformation\", 9, \"values\", 0], lambda x: x.split(\"::\")[1]),\n",
        "    \"freeFrom\": ([\"additionalInformation\", 2, \"values\", 0], lambda x: x),\n",
        "  }\n",
        "\n",
        "  # Extract and transform additional fields based on the extraction configuration\n",
        "  for field, (path, transform) in extraction_config.items():\n",
        "    try:\n",
        "      # Extract the raw value using safe_acces\n",
        "      raw_value = safe_acces(offer, path, default=\"N/A\")\n",
        "      # Apply the transformation if the value is valid\n",
        "      parsed_offer[field] = transform(raw_value) if raw_value != \"N/A\" else \"N/A\"\n",
        "    except (IndexError, KeyError, AttributeError):\n",
        "      # Handle any errors during extraction or transformation\n",
        "      parsed_offer[field] = \"N/A\"\n",
        "\n",
        "  # Return the fully parsed offer details\n",
        "  return parsed_offer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzNOk2a5kdwo"
      },
      "outputs": [],
      "source": [
        "def getter_details(urlstr, qend, qstart):\n",
        "  \"\"\"\n",
        "  Retrieves and parses detailed information for multiple offers from their URLs.\n",
        "  Args:\n",
        "      urlstr (list): A list of URLs for the offers to be processed.\n",
        "  Returns:\n",
        "      list: A list of parsed offer details, each represented as a structured dictionary.\n",
        "  \"\"\"\n",
        "  offer_details = []\n",
        "\n",
        "  for url in urlstr:\n",
        "    print(f\"Downloading {qstart+1} / {qend} of {total}\") # total - total samples qty\n",
        "    try:\n",
        "      # Fetch offer details for the given URL\n",
        "      offers = get_offer_details(url)\n",
        "      # Ensure `offers` is a list; wrap in a list if it's a single dictionary\n",
        "      if isinstance(offers, dict):\n",
        "        offers = [offers]\n",
        "      # Parse each offer and append the details to the result list\n",
        "      for offer in offers:\n",
        "        offer_details.append(parse_offer_details(offer))\n",
        "    except Exception as e:\n",
        "      # Print an error message if something goes wrong for a particular URL\n",
        "      print(f\"Error processing URL {url}: {e}\")\n",
        "    qstart += 1\n",
        "\n",
        "  return offer_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QNmWOtJYG_eZ"
      },
      "outputs": [],
      "source": [
        "#counting quartiles to divide data and divide downloading \n",
        "q = round(len(df)/4)\n",
        "\n",
        "q1end = q\n",
        "q1start = 0\n",
        "q1 = df[\"url\"][q1start:q1end]\n",
        "\n",
        "q2end = q*2\n",
        "q2start = q\n",
        "q2 = df[\"url\"][q2start:q2end]\n",
        "\n",
        "q3end = q*3\n",
        "q3start = q*2\n",
        "q3 = df[\"url\"][q3start:q3end]\n",
        "\n",
        "q4end = #total samples qty\n",
        "q4start = q*3\n",
        "q4 = df[\"url\"][q4start:q4end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E7rar2XYK8EP",
        "outputId": "c1727393-fce1-4a7f-ce8e-cd459cfc8945"
      },
      "outputs": [],
      "source": [
        "# Convert the list of parsed details into a DataFrame and set \"id\" as the index\n",
        "df_details1 = pd.DataFrame(getter_details(q1, q1end, q1start)).set_index(\"id\")\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_dfdetails1.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_details1.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5zeVJRIxK9Qs",
        "outputId": "472de861-3e52-4653-f86a-c2f410f60d15"
      },
      "outputs": [],
      "source": [
        "# Convert the list of parsed details into a DataFrame and set \"id\" as the index\n",
        "df_details2 = pd.DataFrame(getter_details(q2, q2end, q2start)).set_index(\"id\")\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_dfdetails2.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_details2.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6PRIardNK-84",
        "outputId": "e7f6af66-0235-4fce-ca56-a2860c6dab12"
      },
      "outputs": [],
      "source": [
        "# Convert the list of parsed details into a DataFrame and set \"id\" as the index\n",
        "df_details3 = pd.DataFrame(getter_details(q3, q3end, q3start)).set_index(\"id\")\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_dfdetails3.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_details3.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qYGyAhADLAux",
        "outputId": "dfd4eb1e-198b-4595-818c-899c60a84a17"
      },
      "outputs": [],
      "source": [
        "# Convert the list of parsed details into a DataFrame and set \"id\" as the index\n",
        "df_details4 = pd.DataFrame(getter_details(q4, q4end, q4start)).set_index(\"id\")\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_dfdetails4.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_details4.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsodiWxD9HtP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "concat() makes a full copy of the data, and iteratively reusing concat() can create unnecessary copies. Collect all DataFrame or Series objects in a list before using concat().\n",
        "\"\"\"\n",
        "frames = [df_details1, df_details2, df_details3, df_details4]\n",
        "# Combining all offers details\n",
        "df_details_combined = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u-n8oZK_8BP"
      },
      "outputs": [],
      "source": [
        "# Merging all data in to one Data Frame\n",
        "df_combined = pd.merge(df, df_details_combined, how='inner', left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "MWuY9DySpCCZ",
        "outputId": "58fbcd6b-93f7-4608-ef4f-53ec5c397e4a"
      },
      "outputs": [],
      "source": [
        "# Define the path where the file should be saved\n",
        "file_path = '../data/raw_before_anonymization/file_dfcombined.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_combined.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTPILgiqMYv9"
      },
      "source": [
        "# Anonymization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6pXLSRELOoj"
      },
      "outputs": [],
      "source": [
        "# Data frame anonymization\n",
        "df_combined = df_combined.reset_index()\n",
        "df_anonymized = df_combined.drop(columns=[\"id\", \"title\", \"developmentId\", \"url\", \"developerUrl\", \"agencyId\", \"agencyName\"])\n",
        "\n",
        "# Define the path where the file should be saved\n",
        "file_path = '../data/interim/file_dfanonymized.csv'\n",
        "\n",
        "# Save the DataFrame as a CSV file \n",
        "df_anonymized.to_csv(file_path, index=True)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
